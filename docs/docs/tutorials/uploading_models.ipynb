{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJ9jprifq1aH"
      },
      "source": [
        "<div id=\"colab_button\">\n",
        "  <h1>Uploading models</h1>\n",
        "  <a target=\"_blank\" href=\"LINK_GOOGLE_COLAB\"> \n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "</div>\n",
        "\n",
        "_____________________________________________________________________\n",
        "\n",
        "You've tested and installed BlindAI? Time to upload your model on your server instance! \n",
        "\n",
        "You can do it in two lines of code using our Python API, but first your model will need to be converted to the Open Neural Network Exchange Format (ONNX) format. This is because ONNX is a standard enabling framework interoperability, allowing you to easily move models between different machine learning libraries.\n",
        "\n",
        "In this tutorial, we will show you how to take models from three of the most popular ML libraries, `PyTorch`, `TensorFlow` and `HuggingFace`, convert them to ONNX format, and upload them to BlindAI.\n",
        "\n",
        "Let's dive in!\n",
        "\n",
        "## Pre Requisites\n",
        "_______________________________________\n",
        "\n",
        "### Installing required dependencies\n",
        "\n",
        "Unless you're are running this notebook on [Google Colab](LINK_GOOGLE_COLAB), you'll need to have [`Python`](https://www.python.org/downloads/) (3.8 or greater) and [`pip`](https://pypi.org/project/pip/) installed to run this notebook.\n",
        "\n",
        "Then, you'll need to install the BlindAI-preview package."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1QfZ_wCDq1aO"
      },
      "outputs": [],
      "source": [
        "# install blindai-preview package\n",
        "!pip install blindai-preview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7qOIOeGq1aR"
      },
      "source": [
        "We will also need to install some additional dependencies for this notebook:\n",
        "\n",
        "- [`torch`](https://pytorch.org/): to demonstrate ONNX conversion for PyTorch models\n",
        "- [`tensorflow`](https://www.tensorflow.org/), `tensorflow_hub`(https://tfhub.dev/) and `tf2onnx`(https://github.com/onnx/tensorflow-onnx): to demonstrate ONNX conversion for TensorFlow models\n",
        "- [`optimum[exporters]`](https://huggingface.co/): to demonstrate ONNX conversion for HuggingFace models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sj1reMjcq1aS"
      },
      "outputs": [],
      "source": [
        "# install all other required packages\n",
        "!pip install torch tensorflow tf2onnx optimum[exporters]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNqop7P1q1aT"
      },
      "source": [
        "### Launch the BlindAI server\n",
        "\n",
        "Let's launch an instance of BlindAI's server so we can upload the model. \n",
        "\n",
        "For the purposes of this tutorial, we will be using the `blindai_preview.testing` server which has been designed for testing purposes *only*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CeAdSP9sq1aU",
        "outputId": "b5850515-05ee-4d89-edf0-7f13bb3fc7be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BlindAI mock server (version 0.0.8) already installed\n"
          ]
        }
      ],
      "source": [
        "# import testing submodule\n",
        "import blindai_preview.testing\n",
        "\n",
        "# start the server\n",
        "srv = blindai_preview.testing.start_mock_server()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EcUQX6kLq1aV"
      },
      "source": [
        ">Note that the blindai-preview testing module launches the server in simulation mode. As a result, it doesn't provide hardware protection and **must not be used in production**. We created this option to enable users to quickly and easily test BlindAI without needing to run the server on a machine with Intel SGX hardware. To learn more about how to run the server with hardware protections on, see [our documentation](https://blindai-preview.mithrilsecurity.io/en/latest/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwlQXoK5q1aX"
      },
      "source": [
        "## PyTorch model\n",
        "__________________\n",
        "\n",
        "We will use the pretrained `resnet18` neural network as our example model for this section. The `resnet18` model classifies images and returns possbible labels for the image with their probability of being the correct label.\n",
        "\n",
        "### Download the model\n",
        "\n",
        "We'll download the model from `PyTorch Hub`, by using the `hub` module's `load()` method. \n",
        "\n",
        "The first argument we provide specifies the GitHub repo and directory where the model can be installed from. The second specifies the name of the model to be downloaded. Then we set the `pretrained` option to `True`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rqgVfzcIq1aY"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9T_WfYvVq1aZ"
      },
      "source": [
        "Before exporting, it's very important to ensure the model is set to inference mode. We do that by calling `model.eval()` or `model.train(False)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WOroadheq1aa",
        "outputId": "08e40c88-468e-4eb9-c1ca-2fffb54e1015"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvhySJQpq1ab"
      },
      "source": [
        "### ONNX conversion\n",
        "\n",
        "To convert our model to ONNX fomat, we will use the `torch.onnx.export()` function.\n",
        "\n",
        "The `export()` method will execute the model and record a trace of what operators are used to compute the outputs. In order for `export` to be able to perform this \"dry-run\", we need to supply it with some dummy inputs in the shape the model would expect.\n",
        "\n",
        "For `resnet18`, the dummy input should be in the following format: `batch_size`, `channel_width`, `image_size`, `image_size`.\n",
        "\n",
        "The values we provide for each dimension, in this case `batch_size`, `channel_width`, `image_size` and `image_size` will be fixed in the exported ONNX graph, meaning all future input to the model must match this shape. For example, if we set the `batch_size` to 1, users will only be able to upload one image at a time to be inferenced by the model.\n",
        "\n",
        "There is a way to avoid having to set a fixed value though, by specifying that a certain axes is a dynamic axes in the `export` options. For example, if we specify `batch_size` as a dynamic axes, users will be able to upload one or multiple images to the model at a time.\n",
        "\n",
        "Let's create our dummy input now:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-EW3YnrMq1ac"
      },
      "outputs": [],
      "source": [
        "# create dummy inputs for resnet18 model\n",
        "dummy_inputs = torch.zeros(1,3,224,224)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-hAX8Pjq1ac"
      },
      "source": [
        "The channel width should be `3, representing the RGB values of our image, and the image size should be `224x224`. \n",
        "\n",
        "We will set the `batch_size` to `1`, but then specify the first dimension as dynamic with the `dynamic_axes` option.\n",
        "\n",
        "Since the values used for our dummy input are not important in this tutorial, we will set fill them with `0`.\n",
        "\n",
        "Now that we have created our dummy inputs, we are ready to call the `onnx.export()` method. We pass the method:\n",
        "- our PyTorch model,\n",
        "- the dummy inputs,\n",
        "- the name we want to give our ONNX file,\n",
        "- any dynamic axes for input and output values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "avect6dSq1ad",
        "outputId": "a1de917f-595a-4370-a811-4b858cd8e06e"
      },
      "outputs": [],
      "source": [
        "torch.onnx.export(model, dummy_inputs, \n",
        "                \"resnet18.onnx\", \n",
        "                dynamic_axes={'input' : {0 : 'batch_size'}, 'output' : {0 : 'batch_size'}})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10C2iJ8wq1ad"
      },
      "source": [
        "> For information about more export options, see the `torch.onnx.export` [documentation](https://pytorch.org/docs/stable/onnx.html#torch.onnx.export).\n",
        "\n",
        "Our PyTorch model has been succesfully converted!"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "kiekqBNHq1ae"
      },
      "source": [
        "### Upload the model\n",
        "\n",
        "To upload the model, we need to connect to the server using BlindAI's `core.connect()` function.\n",
        "\n",
        "Setting up a production server is not the focus of this tutorial, which is why we've been using a server in `simulation` mode. This means two things: \n",
        "\n",
        "- We also need to set the `simulation_mode` parameter to `True` on the client side. This is needed because the client will refuse to connect to an unsecure server otherwise.  \n",
        "- We also set the `hazmat_http_on_unattested_port` option to `True`. \n",
        "\n",
        "  >By default, the `blindai_preview` package requires a HTTPS connection for communications between the client and server on the unattested port 9923. But for testing purposes we opt out of this requirement and connect without a secure connection. This should **not be done in production**, please refer to our documentation to set up a production server."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rm9SJA1Wq1ag",
        "outputId": "1710e681-ef68-4cc3-e5ed-60879ff1d920"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/blindai_preview/client.py:506: SimulationModeWarning: BlindAI is running in simulation mode. This mode is provided solely for testing purposes. It does not provide any security since there is no SGX enclave. The simulation mode MUST NOT be used in production.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import blindai_preview \n",
        "import blindai_preview\n",
        "\n",
        "# AI company connects\n",
        "client_1 = blindai_preview.core.connect(addr=\"localhost\", simulation_mode=True, hazmat_http_on_unattested_port=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3E6s9iKqq1ag"
      },
      "source": [
        "> For the purposes of this demo, we are running the server on `localhost` using the default ports, but you can modify the host and ports in the `connect()` function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-19O_OO_q1ag"
      },
      "source": [
        "Finally, we can upload the model using the client `upload_model()` method! \n",
        "\n",
        "We'll need to specify the ONNX model's file name via the `model` parameter.\n",
        "\n",
        "Then we can store and print out our `model_id`, which is used to identify the model when running or deleting the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xV7GKVAZq1ah",
        "outputId": "0eeb5099-477b-42e1-b2db-bc2df98c7aee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "b20a226b-7f45-4361-98a4-71486362a53e\n"
          ]
        }
      ],
      "source": [
        "# AI company uploads model to server\n",
        "response = client_1.upload_model(model=\"./resnet18.onnx\")\n",
        "PYTORCH_MODEL_ID = response.model_id\n",
        "print(PYTORCH_MODEL_ID)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "miqufa-aq1ah"
      },
      "source": [
        "The PyTorch model has now been successfully uploaded to the BlindAI server and is ready to be consumed by users."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5xhaka6q1ah"
      },
      "source": [
        "## TensorFlow model\n",
        "__________________________\n",
        "\n",
        "Let's now take a look at how we can convert `TensorFlow saved models` to ONNX using the [`tf2onnx.convert` tool](https://github.com/onnx/tensorflow-onnx). This is the recommended and most popular way to convert TensorFlow models to ONNX, but you can also convert to ONNX from different formats like `graphdef` or `checkpoint` format.\n",
        "\n",
        ">For more information about converting TensorFlow models from `checkpoints` or `graphdef` format, please see the [`tf2onnx` documentation](https://github.com/onnx/tensorflow-onnx).\n",
        "\n",
        "### Loading the model\n",
        "\n",
        "We'll start by loading the built-in ResNet50 model, which is a variation on the ResNet18 model which is 50 layers deep, rather than 18.\n",
        "\n",
        "By importing and initializing the ResNet50 class from the `keras.applications.resnet50` module, we get back a Keras model instance of the ResNet50 model. We then transform this into the SavedModel format by using the `save()` method and providing a name for our `SavedModel` directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U3QXOPSottS5",
        "outputId": "f1fd5b99-6070-4dd0-e4f6-30a4e23d6833"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.applications.resnet50 import ResNet50\n",
        "\n",
        "# initalize pre-trained ResNet50 model\n",
        "model = ResNet50(weights='imagenet')\n",
        "\n",
        "# convert model to SavedModel format\n",
        "model.save(\"my_model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2p66moTJkLS"
      },
      "source": [
        "A `SavedModel` is a directory, containing a `saved_model.pb` file where the TensorFlow model is stored, as well as any additional required files. Here we create our `SavedModel` directory in our current working directory and call it \"my model\".\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kd6fEAqTq1aj"
      },
      "source": [
        "### ONNX Conversion\n",
        "\n",
        "Now that we our model in `SavedModel` format we can use the `tf2onnx.convert` tool to convert our model to ONNX. We provide the path to our `saved-model` and the path we want our `output` onnx file to have."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NHb9MB6Aq1aj",
        "outputId": "010c6c31-b65a-42f7-89e7-60e79e0eea43"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-03-21 17:19:39.613849: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2023-03-21 17:19:39.614001: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2023-03-21 17:19:39.614050: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "/usr/lib/python3.9/runpy.py:127: RuntimeWarning: 'tf2onnx.convert' found in sys.modules after import of package 'tf2onnx', but prior to execution of 'tf2onnx.convert'; this may result in unpredictable behaviour\n",
            "  warn(RuntimeWarning(msg))\n",
            "2023-03-21 17:19:42,630 - WARNING - '--tag' not specified for saved_model. Using --tag serve\n",
            "2023-03-21 17:19:49,967 - INFO - Signatures found in model: [serving_default].\n",
            "2023-03-21 17:19:49,967 - WARNING - '--signature_def' not specified, using first signature: serving_default\n",
            "2023-03-21 17:19:49,969 - INFO - Output names: ['predictions']\n",
            "2023-03-21 17:19:58,055 - INFO - Using tensorflow=2.11.0, onnx=1.12.0, tf2onnx=1.13.0/2c1db5\n",
            "2023-03-21 17:19:58,055 - INFO - Using opset <onnx, 13>\n",
            "2023-03-21 17:19:58,839 - INFO - Computed 0 values for constant folding\n",
            "2023-03-21 17:20:01,327 - INFO - Optimizing ONNX model\n",
            "2023-03-21 17:20:02,733 - INFO - After optimization: Add -1 (18->17), BatchNormalization -53 (53->0), Const -161 (271->110), GlobalAveragePool +1 (0->1), Identity -2 (2->0), ReduceMean -1 (1->0), Squeeze +1 (0->1), Transpose -213 (214->1)\n",
            "2023-03-21 17:20:02,884 - INFO - \n",
            "2023-03-21 17:20:02,884 - INFO - Successfully converted TensorFlow model ./my_model to ONNX\n",
            "2023-03-21 17:20:02,884 - INFO - Model inputs: ['input_1']\n",
            "2023-03-21 17:20:02,884 - INFO - Model outputs: ['predictions']\n",
            "2023-03-21 17:20:02,885 - INFO - ONNX model is saved at ./resnet_tf.onnx\n"
          ]
        }
      ],
      "source": [
        "# convert SavedModel to onnx\n",
        "!python -m tf2onnx.convert --saved-model ./my_model --output ./resnet_tf.onnx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zg6j2YArLSkb"
      },
      "source": [
        "We now have a `resnet_tf.onnx` file in our current working directory ready to be uploaded to BlindAI."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfViVf2Pq1aj"
      },
      "source": [
        "### Upload the model\n",
        "\n",
        "We are now ready to upload the model to the BlindAI server using the `upload_model` method.\n",
        "\n",
        "We then print out the model's ID which would be used later to identify the model when performing operations in the BlindAI API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SL_iJU_Qq1aj",
        "outputId": "3a721469-cbec-45b2-a29f-8ed471988bd7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/blindai_preview/client.py:506: SimulationModeWarning: BlindAI is running in simulation mode. This mode is provided solely for testing purposes. It does not provide any security since there is no SGX enclave. The simulation mode MUST NOT be used in production.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "41608476-bb41-4dda-a7d2-b915db35f5bc\n"
          ]
        }
      ],
      "source": [
        "# AI company uploads model to server\n",
        "client_1 = blindai_preview.core.connect(addr=\"localhost\", simulation_mode=True, hazmat_http_on_unattested_port=True)\n",
        "response = client_1.upload_model(model=\"./resnet_tf.onnx\")\n",
        "TF_MODEL_ID = response.model_id\n",
        "print(TF_MODEL_ID)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNmrFm8Nq1ak"
      },
      "source": [
        "## HuggingFace model\n",
        "_______________________________\n",
        "\n",
        "\n",
        "`HuggingFace` provides a space where the AI community can share and collaborate on open-source models. \n",
        "\n",
        "In this section, we will show you how you can export models from the `HuggingFace hub`, directly in ONNX format. \n",
        "\n",
        "### Download and convert the model\n",
        "\n",
        "If you are using a `PyTorch` or `TensorFlow` model downloaded from the `HuggingFace hub`, you could also use the conversion methods described in the [`PyTorch`](#pytorch-model) or [`TensorFlow`](#tensorflow-model) sections respectively.\n",
        "\n",
        "But we will focus here on the method recommended in the `HuggingFace` documentation, which uses the `optimum` CLI to convert `\"Transformer\"` and `\"Diffuser\"` models to `ONNX` format.\n",
        "\n",
        ">Note that not all architectures can be converted to ONNX format. For a full list of architectures compatible with ONNX conversion, see [the HuggingFace documentation](https://huggingface.co/docs/transformers/serialization#onnx).\n",
        "\n",
        "We'll use `optimum-cli` to download and convert the `resnet50` model from the `HuggingFace hub` to `ONNX`. We'll do so by specifying the `model id` we wish to download and a path where we want all generated files to be stored."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tW4dn92oq1ak",
        "outputId": "928babf6-a463-49d3-cab3-a7d4c39006af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-03-21 17:20:24.957761: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2023-03-21 17:20:24.957889: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2023-03-21 17:20:24.957913: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "2023-03-21 17:20:34.601177: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2023-03-21 17:20:34.601294: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2023-03-21 17:20:34.601338: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "Framework not specified. Using pt to export to ONNX.\n",
            "Downloading (…)lve/main/config.json: 100% 473/473 [00:00<00:00, 61.7kB/s]\n",
            "Downloading pytorch_model.bin: 100% 261M/261M [00:01<00:00, 218MB/s]\n",
            "Automatic task detection to question-answering.\n",
            "Downloading (…)okenizer_config.json: 100% 29.0/29.0 [00:00<00:00, 9.97kB/s]\n",
            "Downloading (…)solve/main/vocab.txt: 100% 213k/213k [00:00<00:00, 19.2MB/s]\n",
            "Downloading (…)/main/tokenizer.json: 100% 436k/436k [00:00<00:00, 8.60MB/s]\n",
            "Using framework PyTorch: 1.13.1+cu116\n",
            "/usr/local/lib/python3.9/dist-packages/transformers/models/distilbert/modeling_distilbert.py:223: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
            "  mask, torch.tensor(torch.finfo(scores.dtype).min)\n",
            "Validating ONNX model huggingface_onnx/model.onnx...\n",
            "\t-[✓] ONNX model output names match reference model (end_logits, start_logits)\n",
            "\t- Validating ONNX Model output \"start_logits\":\n",
            "\t\t-[✓] (2, 16) matches (2, 16)\n",
            "\t\t-[✓] all values close (atol: 0.0001)\n",
            "\t- Validating ONNX Model output \"end_logits\":\n",
            "\t\t-[✓] (2, 16) matches (2, 16)\n",
            "\t\t-[✓] all values close (atol: 0.0001)\n",
            "The ONNX export succeeded and the exported model was saved at: huggingface_onnx\n"
          ]
        }
      ],
      "source": [
        "# Download model from hub in ONNX format\n",
        "!optimum-cli export onnx --model distilbert-base-cased-distilled-squad huggingface_onnx/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHIovqOAq1ak"
      },
      "source": [
        "This will generate various files to the specified location including a `model.onnx` file which contains the downloaded model in `ONNX` format. This method also works with both `PyTorch` and `TensorFlow` models.\n",
        "\n",
        ">Note that, when downloading models from the hub, the optimum-cli `onnx` export tool will perform automatic task detect. We can see this in the output of the command. If you want to turn automatic detection off or for local models, you can use the `--task` option to turn off automatic task detection and specify a task.\n",
        "\n",
        ">*For additional information, see the [HuggingFace documentation](https://huggingface.co/docs/optimum/main/en/exporters/onnx/usage_guides/export_a_model#selecting-a-task).*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CguJiU9q1al"
      },
      "source": [
        "### Upload the model\n",
        "\n",
        "Let's upload this model to the BlindAI server! We use the `upload_model()` method.\n",
        "\n",
        "We then print out the model's ID which will be used later to identify the model when performing operations in the BlindAI API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SMmobv_Kq1al",
        "outputId": "de92fbf4-9c4c-4c04-f762-9ab4ff56f138"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "82fc6f57-5b28-49db-a677-85ab9e3e238e\n"
          ]
        }
      ],
      "source": [
        "# AI company uploads model to server\n",
        "client_1 = blindai_preview.core.connect(addr=\"localhost\", simulation_mode=True, hazmat_http_on_unattested_port=True)\n",
        "ret = client_1.upload_model(model=\"./huggingface_onnx/model.onnx\")\n",
        "HF_MODEL_ID = ret.model_id\n",
        "print(HF_MODEL_ID)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kL2WiCVWq1al"
      },
      "source": [
        "The model has now been successfully uploaded to the BlindAI server and is ready to be consumed by users."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rkpY7ulq1am"
      },
      "source": [
        "## Conclusions\n",
        "____________________________________\n",
        "\n",
        "This is the end of our tutorial on uploading models to BlindAI.\n",
        "\n",
        "We have seen how to:\n",
        "\n",
        "* **Convert** TensorFlow, PyTorch and HuggingFace model to ONNX format.\n",
        "* **Upload** these models to BlindAI.\n",
        "\n",
        "Please check out the rest of our [BlindAI documentation](https://blindai-preview.mithrilsecurity.io/en/latest/) to see more examples of how you can use BlindAI to deploy AI models without compromising the safety of user data or models."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
